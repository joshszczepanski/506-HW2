---
title: "506-HW2"
format: html
author: Josh Szczepanski
---

```{r setup, include=TRUE}
#1

# Version 1: Using a loop

# Define function to simulate a random walk with n steps
random_walk1 <- function(n_steps) {
  
  # Set starting position at 0
  position <- 0
  
  # Loop through each step from 1 to n_steps
  for (i in seq_len(n_steps)) {
    
    # Randomly choose direction: +1 or -1
    direction <- sample(c(1, -1), size = 1) 
    
    if (direction == 1) {
    
      # Set 5% chance of moving forward 10
      paces <- if (runif(1) < 0.05) 10 else 1
    } else {
      
      # Set 20% chance of moving back 3
      paces <- if (runif(1) < 0.20) -3 else -1
    }
    
    # Update position
    position <- position + paces
  }
  
  # Show finishing place
  return(position)
}

# Version 2: Using built-in R vectorized functions.

# Attribution of sources note for following function: Used Microsoft copilot to help me identify ifelse and runif functions as good options

random_walk2 <- function(n_steps) {
  
  # Determine direction
  direction <- sample(c(1, -1), size = n_steps, replace = TRUE)
  
  # Initialize vector to store actual movement values
  paces <- numeric(n_steps)
  
  # Isolate where direction is +1
  forward <- which(direction == 1)
  
  # Create 5% chance of moving 10 steps
  extra_forward <- runif(length(forward)) <= 0.05
  
  # Move proper number of steps
  paces[forward] <- ifelse(extra_forward, 10, 1)
  
  # Isolate where direction is -1
  backwards <- which(direction == -1)
  
  # Create 20% chance of moving 3 steps back
  extra_backwards <-runif(length(backwards)) < 0.20
  
  # Move proper number of steps
  paces[backwards] <- ifelse(extra_backwards, -3, -1)
  
  # Return the total position after all steps
  return(sum(paces))
  
}

# Version 3: Using an apply function
random_walk3 <- function(n_steps) {
  steps <- sapply(seq_len(n_steps), function(i) {
    
    # Choose direction: +1 or -1
    direction <- sample(c(1, -1), size = 1)
    
    # If direction is +1, apply 5% chance to move +10 instead
    if (direction == 1) {
      if (runif(1) < 0.05) return(10) else return(1)
    } else {
      
      # If direction is -1, apply 20% chance to move -3 instead
      if (runif(1) < 0.20) return(-3) else return(-1)
    }
  })
  
  # Return the total position after summing all steps
  return(sum(steps))
}

# Control the randomization
set.seed(32)

# Demonstrate that all versions work
random_walk1(10)
random_walk2(10)
random_walk3(10)
random_walk1(1000)
random_walk2(1000)
random_walk3(1000)


library(microbenchmark)

microbenchmark(
  
  # Run version 1 with 1000 steps
  loop = random_walk1(1000),
  
  # Run version 2 with 1000 steps
  vectorized = random_walk2(1000),
  
  # Run version 3 with 1000 steps
  apply = random_walk3(1000),
  times = 10
)


microbenchmark(
  
  # Run version 1 with 100,000 steps
  loop = random_walk1(100000),
  
  # Run version 2 with 100,000 steps
  vectorized = random_walk2(100000),
  
  # Run version 3 with 100,000 steps
  apply = random_walk3(100000),
  times = 10
)

# Comparing the speeds at low inputs and high inputs: As expected, the high inputs take longer than the slow inputs.. This is evident because the high input times are measured in milliseconds, while the low input times are measured in microseconds.

# Within each input ranges, the speeds are fastest in version 2 (using built in R vectorized functions), with version 1 coming in second and version 3 being the slowest


# Build Monte Carlo simulation function, in other words, use a large number of trials and see how many times we get our desired output. Due to the large number of trials, our result should closely mirror the true probability of landing at 0
prob_zero <- function(n_steps, n_trials = 10000, walk_fn =

# Use version 2 for its speed                                     
random_walk2){
  results <- replicate(n_trials, walk_fn(n_steps))
  mean(results == 0)
}

# Estimate probabilities for different step sizes
set.seed(32)
prob_10   <- prob_zero(10)
prob_100  <- prob_zero(100)
prob_1000 <- prob_zero(1000)

# Display results
prob_10
# The probability of landing at zero when starting at 10 is about 13%

prob_100
# The probability of landing at zero when starting at 100 is about 2.1%

prob_1000
# The probability of landing at zero when starting at 1000 is about 0.5%

#2

# Build a Monte Carlo Sim (high n)
day_sim <- function(n_days = 10000) {

  # Attribution of sources note for following function: Used Microsoft copilot to help me figure out that multiplying n_days by the number of hours is the way to encompass how long each time period lasts
   # Simulate hours with poisson distribution
  morning <- rpois(n_days * 7, lambda = 1)
  daytime <- rpois(n_days * 7, lambda = 8)
  evening <- rpois(n_days * 5, lambda = 12)
  rush_hour <- rnorm(n_days * 2, mean = 60, sd = sqrt(12))


  # Combine all hours into daily totals
  full_day <- matrix(c(morning, daytime, evening, rush_hour), nrow = n_days)
  # Add the daily totals together  
  daily_total <- rowSums(full_day)
  # Find the average
  return(mean(daily_total))
}


estimate <- day_sim()
print(estimate)

# The estimation for average numbers of cars that pass this intersection per day is 243.13



#3

# Load the dataset
youtube <- read.csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-03-02/youtube.csv')



# Subtract off identifying columns
youtube_de_identified <- subset(youtube, select = -c(brand, superbowl_ads_dot_com_url, 	
title, description, channel_title, published_at, youtube_url))

# 3a) Report the dimensions of the data after removing these columns.
dim(youtube_de_identified)
library(ggplot2)

# List of variables to examine
vars <- c("view_count", "like_count", "dislike_count", "favorite_count", "comment_count")

# Examine view count distribution
ggplot(youtube_de_identified, aes(x = view_count)) + geom_histogram(bins = 100, fill = "blue", color = "white") 
# Examine like count distribution
ggplot(youtube_de_identified, aes(x = like_count)) + geom_histogram(bins = 100, fill = "blue", color = "white") 
# Examine dislike count distribution
ggplot(youtube_de_identified, aes(x = dislike_count)) + geom_histogram(bins = 100, fill = "blue", color = "white") 
# Examine favorite count distribution
ggplot(youtube_de_identified, aes(x = favorite_count)) + geom_histogram(bins = 100, fill = "blue", color = "white") 
# Examine comment count distribution
ggplot(youtube_de_identified, aes(x = comment_count)) + geom_histogram(bins = 100, fill = "blue", color = "white") 
# Based on the histograms, the "view_count", "like_count", "dislike_count", and "comment_count seem like variables that could use a transformation prior to being used as the outcome in a linear regression model, and the "favorite_count" seems like a variable that would not be appropriate to use as the outcome in a linear regression model, because that column is composed entirely of 0s and NAs.


# Attribution of Sources Note: Used Microsoft Copilot to identify log as a smart transformation, and to help me set axes and minimal theme

# Loop through each count variable
for (v in c("view_count", "like_count", "dislike_count", "comment_count")) {
  
  # Create and print histogram with log-scaled x-axis
  print(
     # Set variable for x-axis
    ggplot(youtube_de_identified, aes_string(x = v)) +  
      # Plot histogram
      geom_histogram(bins = 100, fill = "blue", color = "white") +  
      # Use log scale to handle skewness
      scale_x_log10() +  
      # Apply clean theme
      theme_minimal()  
  )
}


# Step 1: Log-transform outcome variables
youtube_de_identified$log_view_count <- log1p(youtube_de_identified$view_count)
youtube_de_identified$log_like_count <- log1p(youtube_de_identified$like_count)
youtube_de_identified$log_dislike_count <- log1p(youtube_de_identified$dislike_count)
youtube_de_identified$log_comment_count <- log1p(youtube_de_identified$comment_count)

# Step 2: Define predictors
predictors <- c("funny", "show_product_quickly", "patriotic", "celebrity", "danger", "animals","use_sex", "year")
formula_base <- paste(predictors, collapse = " + ")

# Step 3: Fit models
model_view    <- lm(as.formula(paste("log_view_count ~", formula_base)), data = youtube_de_identified)
model_like    <- lm(as.formula(paste("log_like_count ~", formula_base)), data = youtube_de_identified)
model_dislike <- lm(as.formula(paste("log_dislike_count ~", formula_base)), data = youtube_de_identified)
model_comment <- lm(as.formula(paste("log_comment_count ~", formula_base)), data = youtube_de_identified)

# Step 4: Review summaries
summary(model_view)
summary(model_like)
summary(model_dislike)
summary(model_comment)

# Aside from year and intercept, the result with the lowest p-value, indicating potential statistical significance comes from the "patriotic" binary flag in the "dislike count" regression chart. This p-value is 0.053.  The coefficient is positive, indicating that the commercials that were patriotic had an increased number of dislikes.



# Attribution of Sources note: When I first tried calculating Beta this way I was getting a column of only NA. I used Microsoft Copilot to help me realize I needed to clean the data to fix that issue.

# Load necessary library
library(dplyr)

# Create design matrix X
X <- model.matrix(~ funny + show_product_quickly + patriotic + celebrity + danger +  animals + use_sex + year, data = youtube_de_identified)

# Create response variable y
y <- youtube_de_identified$log_view_count

# Test to see if there is missing data
sum(!complete.cases(X, y)) 

# Get rid of the missing data
complete_rows <- complete.cases(X, y)
X_clean <- X[complete_rows, ]
y_clean <- y[complete_rows]

# Use OLS
beta_hat <- solve(t(X_clean) %*% X_clean) %*% t(X_clean) %*% y_clean

# Display results
beta_hat



```














